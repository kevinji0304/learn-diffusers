{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment and Dependencies\n",
    "Install and import required libraries including diffusers, torch, datasets, and transformers. Setup training device (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install diffusers torch datasets transformers\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "from diffusers import DDPMPipeline, DDPMScheduler, DDPMPipeline\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Setup training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Custom Dataset\n",
    "Load and preprocess custom images, create a custom dataset class, and set up the data loader for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load custom dataset\n",
    "image_dir = 'path_to_your_image_directory'  # Replace with the path to your image directory\n",
    "dataset = CustomImageDataset(image_dir=image_dir, transform=transform)\n",
    "\n",
    "# Create data loader\n",
    "batch_size = 16\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Display the number of images loaded\n",
    "print(f\"Number of images loaded: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DDPM Model Architecture\n",
    "Configure the DDPM model architecture using UNet2DModel and set hyperparameters like image size, timesteps, and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "# Define DDPM model architecture\n",
    "model = UNet2DModel(\n",
    "    sample_size=256,  # the target image size\n",
    "    in_channels=3,    # number of input channels (3 for RGB images)\n",
    "    out_channels=3,   # number of output channels\n",
    "    layers_per_block=2,  # number of layers per block\n",
    "    block_out_channels=(64, 128, 256, 512),  # number of output channels for each block\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),  # types of down blocks\n",
    "    up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"AttnUpBlock2D\")  # types of up blocks\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "num_timesteps = 1000  # number of timesteps for the diffusion process\n",
    "learning_rate = 1e-4  # learning rate for the optimizer\n",
    "num_epochs = 10  # number of training epochs\n",
    "\n",
    "# Move model to the training device\n",
    "model.to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Pipeline\n",
    "Set up the DDPMPipeline for training, including noise scheduler and optimizer configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the noise scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=num_timesteps)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        # Move batch to the training device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn(batch.shape).to(device)\n",
    "        \n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, num_timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        # Add noise to the images according to the noise scheduler\n",
    "        noisy_images = noise_scheduler.add_noise(batch, noise, timesteps)\n",
    "        \n",
    "        # Predict the noise residual\n",
    "        noise_pred = model(noisy_images, timesteps).sample\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {step}/{len(data_loader)}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Implement the training loop with forward pass, loss calculation, and backward propagation. Include progress tracking and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with progress tracking and model checkpointing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        # Move batch to the training device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn(batch.shape).to(device)\n",
    "        \n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, num_timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        # Add noise to the images according to the noise scheduler\n",
    "        noisy_images = noise_scheduler.add_noise(batch, noise, timesteps)\n",
    "        \n",
    "        # Predict the noise residual\n",
    "        noise_pred = model(noisy_images, timesteps).sample\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Print loss every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {step}/{len(data_loader)}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_epoch_loss}\")\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = f\"ddpm_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Model checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Test Model\n",
    "Save the trained model and demonstrate how to generate new images using the custom-trained pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model\n",
    "final_model_path = \"ddpm_final.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "\n",
    "# Load the trained model for inference\n",
    "model.load_state_dict(torch.load(final_model_path))\n",
    "model.eval()\n",
    "\n",
    "# Define a function to generate new images\n",
    "def generate_images(model, num_images=1, num_inference_steps=120):\n",
    "    pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "    pipeline.to(device)\n",
    "    generated_images = []\n",
    "    for _ in range(num_images):\n",
    "        image = pipeline(num_inference_steps=num_inference_steps).images[0]\n",
    "        generated_images.append(image)\n",
    "    return generated_images\n",
    "\n",
    "# Generate and display new images\n",
    "num_images_to_generate = 5\n",
    "generated_images = generate_images(model, num_images=num_images_to_generate)\n",
    "\n",
    "# Display the generated images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, num_images_to_generate, figsize=(15, 5))\n",
    "for i, img in enumerate(generated_images):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
